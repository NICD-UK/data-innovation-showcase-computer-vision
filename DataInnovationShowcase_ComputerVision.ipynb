{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu2_ZlFe0RXo"
      },
      "source": [
        "# Computer vision with PyTorch\n",
        "\n",
        "- Task: Image classification - Task of assigning a label or class to an entire image.\n",
        "\n",
        "- Data: FashionMNIST - A dataset of Zalando's (online retailer) garment images with their labels.\n",
        "\n",
        "- Model: A very simple model of three layers for illustrative purposes.\n",
        "\n",
        "- Framework: PyTorch - A machine learning framework based on the Torch library, used for creating deep neural networks applications, such as computer vision tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVaKdSp5Cmxf"
      },
      "source": [
        "![](https://github.com/NICD-UK/data-innovation-showcase-computer-vision/blob/main/figures/ComputerVisionTask.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06lTb2TUw-Lp"
      },
      "source": [
        "## Step 1. Import useful libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU6_84AegG87"
      },
      "outputs": [],
      "source": [
        "# Import PyTorch: Framework for developing code for artificial neural network models\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Import torchvision that consists of popular datasets, model architectures, and common image transformations for computer vision.\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Import matplotlib for visualisations (static, animated, and interactive)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import tqdm for progress bar (loops show a progress meter)\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Generate random numbers\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfCQns1ZxJpT"
      },
      "source": [
        "## Step 2. Set-up the train and the test data\n",
        "\n",
        "In this example we are using the FashionMNIST dataset.\n",
        "\n",
        "It has a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
        "\n",
        "---\n",
        "**Note:**\n",
        "\n",
        "The FashionMNIST is a typical dataset used for benchmarking in computer vision and it exists in the `torchvision.datasets` library, hence we will load it from there. In real world applications where we have our own data we have to load them from a storage space or an online source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjcumWeeghVz"
      },
      "outputs": [],
      "source": [
        "# Set-up training data - Look at the data folder that will be created and its contents\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\",  # location where the downloaded data should be stored\n",
        "    train=True,  # get the training data\n",
        "    download=True,  # download the data in the right folder if they don't already exist\n",
        "    transform=ToTensor(),  # transform images from PIL format to Torch tensors\n",
        "    target_transform=None,  # apply transformation to labels\n",
        ")\n",
        "\n",
        "# Setup testing data\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\", train=False, download=True, transform=ToTensor()  # get the test data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVNIutVvg35U"
      },
      "outputs": [],
      "source": [
        "# Check the size of the train and test data\n",
        "len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwLFcLiH4N_f"
      },
      "outputs": [],
      "source": [
        "# Look at the type of the train and test data\n",
        "print(type(train_data))\n",
        "print(type(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIWle6AO4XAr"
      },
      "source": [
        "Both datasets have the type `torch.dataset`.\n",
        "This type stores the samples and their corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJzhCVKag_eT"
      },
      "outputs": [],
      "source": [
        "# What are the available 10 classes of garments?\n",
        "class_names = train_data.classes\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeogJZgCglzh"
      },
      "outputs": [],
      "source": [
        "# Let's look at the type of a sample image, its content and its shape\n",
        "image, label = train_data[0]\n",
        "image, label\n",
        "\n",
        "print(\"The type of the image is a tensor:\", type(image))\n",
        "print(image)\n",
        "print(label)\n",
        "\n",
        "print(image.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEidRxPGgwYD"
      },
      "outputs": [],
      "source": [
        "# Let's visualise one image\n",
        "image, label = train_data[3]\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "title = plt.title(class_names[label])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTTaJusjxXbJ"
      },
      "source": [
        "### Create the dataloaders\n",
        "\n",
        "The `torch.dataset` type stores the samples and their corresponding labels. However, while training a model we need to be able to handle it efficiently e.g. iterate over it and split it in batches.\n",
        "\n",
        "Hence, we convert the Dataset to a Dataloader. A DataLoader wraps an iterable around the Dataset to enable easy access to the samples and help with the internal processing that happens in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxB0dBZEhtLF"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Turn datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(\n",
        "    train_data,  # dataset to turn into iterable\n",
        "    batch_size=BATCH_SIZE,  # how many samples per batch?\n",
        "    shuffle=True,  # shuffle data every epoch?\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,  # don't necessarily have to shuffle the testing data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-w5GutohwQe"
      },
      "outputs": [],
      "source": [
        "# Let's check out what we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK8XB87hh0dH"
      },
      "outputs": [],
      "source": [
        "# Check out what's inside the training dataloader\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuPdgoS2iYX7"
      },
      "source": [
        "## Step 3. Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O2hb8FlxvjU"
      },
      "source": [
        "### a. Identify the available device\n",
        "\n",
        "Deep learning models perform thousands of computations and require high speed. GPUs (Graphics Processing Unit) enable multiple, simultaneous computations and speed up the training process. Hence, we need to check if a GPU is available so as to use it, else we proceed with our CPU.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "In the simple examples of the widely used benchmark datasets and simple models training on a CPU will be completed quite quickly. In real-world problems training even on a GPU might require hours or even days so performing this on a CPU would be inefficient and time-consuming, and could even fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLTKmdJNjq-8"
      },
      "outputs": [],
      "source": [
        "# Setup device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abKBP4NQx3xE"
      },
      "source": [
        "### b. Define the model\n",
        "\n",
        "First, for illustrative purposes, in the cell below we define a very very simple model (would never be used in real life problems) that consists of a single layer to see how passing the input through a layer affects its shape. Next, we will create a 'more complex' model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvBq3UObiang"
      },
      "outputs": [],
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = (\n",
        "    nn.Flatten()\n",
        ")  # all nn modules function as a model (can do a forward pass)\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x)  # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdnzW_x_yPe0"
      },
      "source": [
        "Now, let's define a more 'complex' model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tkKUF6oihBx"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class FashionMNISTBaselineModel(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(),  # neural networks like their inputs in vector form\n",
        "            nn.Linear(\n",
        "                in_features=input_shape, out_features=hidden_units\n",
        "            ),  # in_features = number of features in a data sample (784 pixels)\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer_stack(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ouZHhpyZG_"
      },
      "source": [
        "After we defined the model class, we will instantiate an object of that class, by passing the appropriate parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz_OHiyMiv2S"
      },
      "outputs": [],
      "source": [
        "# Need to setup model with input parameters\n",
        "model_0 = FashionMNISTBaselineModel(\n",
        "    input_shape=784,  # one for every pixel (28x28)\n",
        "    hidden_units=10,  # how many units in the hidden layer\n",
        "    output_shape=len(class_names),  # one for every class\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDBPpC_YynFb"
      },
      "source": [
        "### c. Setup the loss and the optimiser\n",
        "\n",
        "Cross entropy loss is a metric used in machine learning to measure how well a classification model performs. It is a number between 0 and 1. The closer to 0 the better the model.\n",
        "\n",
        "The optimizer is an algorithm that adjusts the parameters of the neural network (e.g. weights, biases, learning rate), in order to reduce the overall loss and improve the accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky8BV0OVi87K"
      },
      "outputs": [],
      "source": [
        "# Setup loss function and optimizer\n",
        "loss_fn = (\n",
        "    nn.CrossEntropyLoss()\n",
        ")  # this is also called \"criterion\"/\"cost function\"\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEXQY5l2y4kU"
      },
      "source": [
        "### d. Define the accuracy\n",
        "\n",
        "The accuracy is used to measure how well the model performs. It calculates how many correct predictions we had."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dTq8Xc7j16t"
      },
      "outputs": [],
      "source": [
        "# Calculate the accuracy (a classification metric)\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): Truth labels for predictions.\n",
        "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
        "\n",
        "    Returns:\n",
        "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
        "    \"\"\"\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ6MZqES-Pji"
      },
      "source": [
        "**Note** The *accuracy* measures the percentage of correct predictions made by a model, while the *Cross Entropy* measures the difference between the predicted output and the ground truth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7cSulVKzlpL"
      },
      "source": [
        "### e. Create the training and the test loop\n",
        "\n",
        "This is where we train and test our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJiMH3B7jNgr"
      },
      "outputs": [],
      "source": [
        "# Set the number of epochs\n",
        "epochs = 3 # We use a small number to have faster training time.\n",
        "\n",
        "model_0.to(device)\n",
        "\n",
        "# Create training and testing loop\n",
        "for epoch in tqdm(range(epochs)): #tqdm visualises the progress of the loop\n",
        "    print(f\"Epoch: {epoch}\\n\")\n",
        "\n",
        "    ### Training loop\n",
        "    train_loss = 0\n",
        "\n",
        "    for X, y in train_dataloader:\n",
        "        # print(batch)\n",
        "\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        model_0.train()\n",
        "\n",
        "        # 1. Forward pass - calculation of the values of the output layers from the input data\n",
        "        y_pred = model_0(X)\n",
        "        y_pred.to(device)\n",
        "\n",
        "        # 2. Calculate loss (per batch)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss  # accumulate the loss per epoch\n",
        "\n",
        "        # 3. For the Optimizer set gradients of all model parameters to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward - backpropagation (computes dloss/dx for every parameter)\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step (performs a single optimization step - parameter update)\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate the average loss per batch per epoch\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    ### Testing loop\n",
        "    # Initialise the variables that will accumulate loss and accuracy\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    # Layers and parts of the model behave differently during training and inference\n",
        "    model_0.eval()\n",
        "    # Context manager that ensures the operations have no interactions with training characteristics e.g. gradient calculations\n",
        "    with torch.inference_mode():\n",
        "        for X, y in test_dataloader:\n",
        "\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass - calculation of the values of the output layers from the input data\n",
        "            test_pred = model_0(X)\n",
        "            test_pred.to(device)\n",
        "\n",
        "            # 2. Calculate the loss\n",
        "            test_loss += loss_fn(test_pred, y)  # accumulate the loss per epoch\n",
        "\n",
        "            # 3. Calculate the accuracy\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1)) # accumulate per epoch\n",
        "\n",
        "        # Within the torch.inference_mode() calculate the metrics (test loss and test accuracy)\n",
        "        test_loss /= len(test_dataloader) # per batch, as the len(test_dataloader) = no of batches\n",
        "        test_acc /= len(test_dataloader) # per batch\n",
        "\n",
        "    # Print out the train and the test loss, and the accuracy.\n",
        "    # As the epochs progress the losses should be reducing and the accuracy increasing\n",
        "    print(f\"\\nTrain loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {test_acc:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIg9u7Nwzy7-"
      },
      "source": [
        "## Step 4. Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEvnLrJ0kLVA"
      },
      "outputs": [],
      "source": [
        "def make_predictions(model: torch.nn.Module, img: torch.Tensor, device: torch.device = device):\n",
        "    pred_probs = []\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "          # Prepare the sample image\n",
        "          img = torch.unsqueeze(img, dim=0) # Add an extra dimension\n",
        "\n",
        "          # Forward pass to get the predictions\n",
        "          # The model outputs raw logits which are unnormalised and not easy to interpret\n",
        "          pred_logit = model(img)\n",
        "          # print(f\"\\nTrain loss: {pred_logit}\") # Uncomment to see what the logits looks like\n",
        "\n",
        "          # Get the prediction probabilities using Softmax\n",
        "          pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n",
        "\n",
        "          # Remove the predictions from the GPU if you later want to apply additional calculations\n",
        "          pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "    # Stack the pred_probs to turn them into a tensor\n",
        "    return torch.stack(pred_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PecL51dmkUSX"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test samples with the baseline model\n",
        "\n",
        "# Get a random index for an image and select the image from the test_data\n",
        "random_garment_idx = random.randint(0, len(test_data))\n",
        "sample_image, ground_truth_label = test_data[random_garment_idx]\n",
        "\n",
        "# Calculate the predicted probabilities by calling the `make_predictions` method that we previously defined\n",
        "pred_probs = make_predictions(model=model_0.to(device), img=sample_image.to(device))\n",
        "\n",
        "# Get the predicted class which is the one that had the highest probability\n",
        "pred_class = pred_probs.argmax(dim=1)\n",
        "print(f\" The predicted class is: {pred_class.item()}, {class_names[pred_class.item()]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yY4vPBHlGzE"
      },
      "outputs": [],
      "source": [
        "# Check what is the actual ground truth label.\n",
        "# If it is the same as the predicted, the model had a correct prediction.\n",
        "print(f\" The actual class is: {ground_truth_label}, {class_names[ground_truth_label]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz7DlHrXacfY"
      },
      "outputs": [],
      "source": [
        "# Let's visualize the test image\n",
        "image, label = test_data[random_garment_idx]\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "title = plt.title(class_names[label])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzQowFBu0EA_"
      },
      "source": [
        "## Further improvements, additional steps and point to consider\n",
        "\n",
        "- Save the model for further usage using `torch.save()` and saving the `state_dict()` of the model.\n",
        "\n",
        "- Use functions for code re-usability\n",
        "\n",
        "- Use more sophisticated NN architectures\n",
        "\n",
        "- Use experiment tracking e.g. MLFlow\n",
        "\n",
        "- Compare model performance for various models to select the best performing model\n",
        "\n",
        "- Use pre-trained models + transfer learning + fine-tuning to make them more specific to the task of interest\n",
        "\n",
        "- In deployment: track model performance over time, re-train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "This workshop was based on the computer vision tutorial provided by Zero to Mastery Learn PyTorch for Deep Learning course.\n",
        "More information available:\n",
        "\n",
        "- [GitHub repo](https://github.com/mrdbourke/pytorch-deep-learning/) under the MIT licence.\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2021 Daniel Bourke\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n"
      ],
      "metadata": {
        "id": "qgFy6oY3a3qS"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}